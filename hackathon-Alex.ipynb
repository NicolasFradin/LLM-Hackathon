{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f29f28d-6b3a-4f1b-80be-d320b58006e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.vectorstores.utils import filter_complex_metadata # 'filter_complex_metadata' removes complex metadata that are not in str, int, float or bool format\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, LlamaCppEmbeddings\n",
    "from langchain.embeddings import OpenAIEmbeddings # NEED OPENAI KEY\n",
    "from langchain.vectorstores import Chroma, FAISS\n",
    "import bs4\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from os.path import expanduser\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from langchain import hub\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "import pandas as pd \n",
    "import tiktoken\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import (\n",
    "    get_query_constructor_prompt,\n",
    "    StructuredQueryOutputParser,\n",
    ")\n",
    "from langchain.retrievers.self_query.chroma import ChromaTranslator\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.query_constructor.base import (\n",
    "    get_query_constructor_prompt,\n",
    "    load_query_constructor_runnable,\n",
    ")\n",
    "import json\n",
    "import logging\n",
    "# Set logging for the queries\n",
    "logging.basicConfig()\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94651c3b-10d5-46b0-8faf-96b33a41d9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>color</th>\n",
       "      <th>automatic</th>\n",
       "      <th>odometer_range</th>\n",
       "      <th>market_country</th>\n",
       "      <th>number_of_doors</th>\n",
       "      <th>number_of_seats</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>make</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>model</th>\n",
       "      <th>body_type</th>\n",
       "      <th>msrp</th>\n",
       "      <th>extras</th>\n",
       "      <th>avg_daily_price</th>\n",
       "      <th>list_countries</th>\n",
       "      <th>list_cities</th>\n",
       "      <th>list_latitudes</th>\n",
       "      <th>list_longitudes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384926</td>\n",
       "      <td>1384926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>HYBRID</td>\n",
       "      <td>SUBARU</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Crosstrek</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>36345.0</td>\n",
       "      <td>AIR_MATTRESS,CHILD_SAFETY_SEAT,PET_FEE,CHILD_S...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>US,US</td>\n",
       "      <td>El Monte,South El Monte</td>\n",
       "      <td>34.0869000,34.0532200</td>\n",
       "      <td>-118.0196000,-118.0655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1362285</td>\n",
       "      <td>1362285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>JEEP</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Wrangler Unlimited</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>42150.0</td>\n",
       "      <td>AIR_MATTRESS,PREPAID_REFUEL</td>\n",
       "      <td>89.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>34.1812300</td>\n",
       "      <td>-118.4480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1339166</td>\n",
       "      <td>1339166</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>JEEP</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Grand Cherokee L</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>48645.0</td>\n",
       "      <td>AIR_MATTRESS,SLEEPING_BAG,UNLIMITED_MILES,PREP...</td>\n",
       "      <td>117.0</td>\n",
       "      <td>US,US,US,US</td>\n",
       "      <td>Denver,Denver,Aurora,Aurora</td>\n",
       "      <td>39.8329800,39.8213600,39.8198100,39.6833800</td>\n",
       "      <td>-104.7047300,-104.7732900,-104.6877000,-104.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1350801</td>\n",
       "      <td>1350801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>30-40</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Golf SportWagen</td>\n",
       "      <td>WAGON</td>\n",
       "      <td>22725.0</td>\n",
       "      <td>BEACH_GEAR,BEACH_GEAR,BEACH_GEAR,BEACH_GEAR</td>\n",
       "      <td>59.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Kailua</td>\n",
       "      <td>21.3873200</td>\n",
       "      <td>-157.7293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1392963</td>\n",
       "      <td>1392963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>80-90</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Corolla</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>22955.0</td>\n",
       "      <td>BEACH_GEAR,CHILD_SAFETY_SEAT,CHILD_SAFETY_SEAT...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Kaneohe</td>\n",
       "      <td>21.3924400</td>\n",
       "      <td>-157.7970200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  vehicle_id  color  automatic odometer_range market_country  \\\n",
       "0  1384926     1384926    NaN          1           0-10             US   \n",
       "1  1362285     1362285    NaN          1           0-10             US   \n",
       "2  1339166     1339166  WHITE          1           0-10             US   \n",
       "3  1350801     1350801    NaN          1          30-40             US   \n",
       "4  1392963     1392963    NaN          1          80-90             US   \n",
       "\n",
       "   number_of_doors  number_of_seats fuel_type        make  ...    year  \\\n",
       "0              5.0              5.0    HYBRID      SUBARU  ...  2022.0   \n",
       "1              4.0              NaN  GASOLINE        JEEP  ...  2022.0   \n",
       "2              4.0              7.0  GASOLINE        JEEP  ...  2022.0   \n",
       "3              4.0              5.0  GASOLINE  VOLKSWAGEN  ...  2016.0   \n",
       "4              4.0              5.0  GASOLINE      TOYOTA  ...  2015.0   \n",
       "\n",
       "                model      body_type     msrp  \\\n",
       "0           Crosstrek  FOUR_DOOR_SUV  36345.0   \n",
       "1  Wrangler Unlimited  FOUR_DOOR_SUV  42150.0   \n",
       "2    Grand Cherokee L  FOUR_DOOR_SUV  48645.0   \n",
       "3     Golf SportWagen          WAGON  22725.0   \n",
       "4             Corolla          SEDAN  22955.0   \n",
       "\n",
       "                                              extras avg_daily_price  \\\n",
       "0  AIR_MATTRESS,CHILD_SAFETY_SEAT,PET_FEE,CHILD_S...            60.0   \n",
       "1                        AIR_MATTRESS,PREPAID_REFUEL            89.0   \n",
       "2  AIR_MATTRESS,SLEEPING_BAG,UNLIMITED_MILES,PREP...           117.0   \n",
       "3        BEACH_GEAR,BEACH_GEAR,BEACH_GEAR,BEACH_GEAR            59.0   \n",
       "4  BEACH_GEAR,CHILD_SAFETY_SEAT,CHILD_SAFETY_SEAT...            65.0   \n",
       "\n",
       "   list_countries                  list_cities  \\\n",
       "0           US,US      El Monte,South El Monte   \n",
       "1              US                  Los Angeles   \n",
       "2     US,US,US,US  Denver,Denver,Aurora,Aurora   \n",
       "3              US                       Kailua   \n",
       "4              US                      Kaneohe   \n",
       "\n",
       "                                list_latitudes  \\\n",
       "0                        34.0869000,34.0532200   \n",
       "1                                   34.1812300   \n",
       "2  39.8329800,39.8213600,39.8198100,39.6833800   \n",
       "3                                   21.3873200   \n",
       "4                                   21.3924400   \n",
       "\n",
       "                                     list_longitudes  \n",
       "0                          -118.0196000,-118.0655100  \n",
       "1                                       -118.4480300  \n",
       "2  -104.7047300,-104.7732900,-104.6877000,-104.81...  \n",
       "3                                       -157.7293900  \n",
       "4                                       -157.7970200  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars = pd.read_csv(\"cars_details.csv\")\n",
    "print(df_cars.shape)\n",
    "df_cars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8da725f-61fb-43a1-aa03-c06c06956714",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>vehicle_id</th>\n",
       "      <th>color</th>\n",
       "      <th>automatic</th>\n",
       "      <th>odometer_range</th>\n",
       "      <th>market_country</th>\n",
       "      <th>number_of_doors</th>\n",
       "      <th>number_of_seats</th>\n",
       "      <th>fuel_type</th>\n",
       "      <th>make</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>model</th>\n",
       "      <th>body_type</th>\n",
       "      <th>msrp</th>\n",
       "      <th>extras</th>\n",
       "      <th>avg_daily_price</th>\n",
       "      <th>list_countries</th>\n",
       "      <th>list_cities</th>\n",
       "      <th>list_latitudes</th>\n",
       "      <th>list_longitudes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1384926</td>\n",
       "      <td>1384926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>HYBRID</td>\n",
       "      <td>SUBARU</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Crosstrek</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>36345.0</td>\n",
       "      <td>AIR_MATTRESS,CHILD_SAFETY_SEAT,PET_FEE,CHILD_S...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>US,US</td>\n",
       "      <td>El Monte,South El Monte</td>\n",
       "      <td>34.0869000,34.0532200</td>\n",
       "      <td>-118.0196000,-118.0655100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1362285</td>\n",
       "      <td>1362285</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>JEEP</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Wrangler Unlimited</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>42150.0</td>\n",
       "      <td>AIR_MATTRESS,PREPAID_REFUEL</td>\n",
       "      <td>89.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>34.1812300</td>\n",
       "      <td>-118.4480300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1339166</td>\n",
       "      <td>1339166</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>JEEP</td>\n",
       "      <td>...</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>Grand Cherokee L</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>48645.0</td>\n",
       "      <td>AIR_MATTRESS,SLEEPING_BAG,UNLIMITED_MILES,PREP...</td>\n",
       "      <td>117.0</td>\n",
       "      <td>US,US,US,US</td>\n",
       "      <td>Denver,Denver,Aurora,Aurora</td>\n",
       "      <td>39.8329800,39.8213600,39.8198100,39.6833800</td>\n",
       "      <td>-104.7047300,-104.7732900,-104.6877000,-104.81...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1350801</td>\n",
       "      <td>1350801</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>30-40</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>VOLKSWAGEN</td>\n",
       "      <td>...</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>Golf SportWagen</td>\n",
       "      <td>WAGON</td>\n",
       "      <td>22725.0</td>\n",
       "      <td>BEACH_GEAR,BEACH_GEAR,BEACH_GEAR,BEACH_GEAR</td>\n",
       "      <td>59.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Kailua</td>\n",
       "      <td>21.3873200</td>\n",
       "      <td>-157.7293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1392963</td>\n",
       "      <td>1392963</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>80-90</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>TOYOTA</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Corolla</td>\n",
       "      <td>SEDAN</td>\n",
       "      <td>22955.0</td>\n",
       "      <td>BEACH_GEAR,CHILD_SAFETY_SEAT,CHILD_SAFETY_SEAT...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Kaneohe</td>\n",
       "      <td>21.3924400</td>\n",
       "      <td>-157.7970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>1387616</td>\n",
       "      <td>1387616</td>\n",
       "      <td>BLACK</td>\n",
       "      <td>1</td>\n",
       "      <td>70-80</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>GMC</td>\n",
       "      <td>...</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>Yukon</td>\n",
       "      <td>FOUR_DOOR_SUV</td>\n",
       "      <td>67520.0</td>\n",
       "      <td>UNLIMITED_MILES,STROLLER,PREPAID_REFUEL,CAMPIN...</td>\n",
       "      <td>91.0</td>\n",
       "      <td>US,US,US,US,US,US</td>\n",
       "      <td>Erie,Denver,Erie,Denver,Denver,Denver</td>\n",
       "      <td>40.0640500,39.8563600,40.0359200,39.8262900,39...</td>\n",
       "      <td>-105.0611100,-104.6764100,-105.0545600,-104.76...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1314727</td>\n",
       "      <td>1314727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CA</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNLIMITED_MILES,UNLIMITED_MILES</td>\n",
       "      <td>52.0</td>\n",
       "      <td>CA,CA</td>\n",
       "      <td>Montréal,Montréal</td>\n",
       "      <td>45.5657500,45.5226100</td>\n",
       "      <td>-73.6469600,-73.5857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>1323195</td>\n",
       "      <td>1323195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0-10</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UNLIMITED_MILES,UNLIMITED_MILES,CHILD_SAFETY_S...</td>\n",
       "      <td>69.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Grand Prairie</td>\n",
       "      <td>32.7209900</td>\n",
       "      <td>-97.0444800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>1349014</td>\n",
       "      <td>1349014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>60-70</td>\n",
       "      <td>US</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>BMW</td>\n",
       "      <td>...</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>4 Series</td>\n",
       "      <td>COUPE</td>\n",
       "      <td>42500.0</td>\n",
       "      <td>UNLIMITED_MILES,UNLIMITED_MILES,ONE_WAY_TRIP</td>\n",
       "      <td>62.0</td>\n",
       "      <td>US,US,US,US</td>\n",
       "      <td>Myrtle Beach,Sandy Springs,Sandy Springs,Atlanta</td>\n",
       "      <td>33.6822000,33.9941100,33.9940900,33.9946400</td>\n",
       "      <td>-78.9278900,-84.3411200,-84.3411300,-84.3501600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1291</th>\n",
       "      <td>1394439</td>\n",
       "      <td>1394439</td>\n",
       "      <td>RED</td>\n",
       "      <td>1</td>\n",
       "      <td>100-110</td>\n",
       "      <td>US</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>GASOLINE</td>\n",
       "      <td>KIA</td>\n",
       "      <td>...</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>Soul</td>\n",
       "      <td>FOUR_DOOR_HATCHBACK</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>UNLIMITED_MILES,UNLIMITED_MILES,UNLIMITED_MILE...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>US</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>39.3420800</td>\n",
       "      <td>-76.5464900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1292 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  vehicle_id  color  automatic odometer_range market_country  \\\n",
       "0     1384926     1384926    NaN          1           0-10             US   \n",
       "1     1362285     1362285    NaN          1           0-10             US   \n",
       "2     1339166     1339166  WHITE          1           0-10             US   \n",
       "3     1350801     1350801    NaN          1          30-40             US   \n",
       "4     1392963     1392963    NaN          1          80-90             US   \n",
       "...       ...         ...    ...        ...            ...            ...   \n",
       "1287  1387616     1387616  BLACK          1          70-80             US   \n",
       "1288  1314727     1314727    NaN          1          60-80             CA   \n",
       "1289  1323195     1323195    NaN          1           0-10             US   \n",
       "1290  1349014     1349014    NaN          1          60-70             US   \n",
       "1291  1394439     1394439    RED          1        100-110             US   \n",
       "\n",
       "      number_of_doors  number_of_seats fuel_type        make  ...    year  \\\n",
       "0                 5.0              5.0    HYBRID      SUBARU  ...  2022.0   \n",
       "1                 4.0              NaN  GASOLINE        JEEP  ...  2022.0   \n",
       "2                 4.0              7.0  GASOLINE        JEEP  ...  2022.0   \n",
       "3                 4.0              5.0  GASOLINE  VOLKSWAGEN  ...  2016.0   \n",
       "4                 4.0              5.0  GASOLINE      TOYOTA  ...  2015.0   \n",
       "...               ...              ...       ...         ...  ...     ...   \n",
       "1287              4.0              7.0  GASOLINE         GMC  ...  2015.0   \n",
       "1288              0.0              5.0  GASOLINE         NaN  ...     NaN   \n",
       "1289              4.0              5.0  GASOLINE         NaN  ...     NaN   \n",
       "1290              2.0              4.0  GASOLINE         BMW  ...  2014.0   \n",
       "1291              4.0              5.0  GASOLINE         KIA  ...  2013.0   \n",
       "\n",
       "                   model            body_type     msrp  \\\n",
       "0              Crosstrek        FOUR_DOOR_SUV  36345.0   \n",
       "1     Wrangler Unlimited        FOUR_DOOR_SUV  42150.0   \n",
       "2       Grand Cherokee L        FOUR_DOOR_SUV  48645.0   \n",
       "3        Golf SportWagen                WAGON  22725.0   \n",
       "4                Corolla                SEDAN  22955.0   \n",
       "...                  ...                  ...      ...   \n",
       "1287               Yukon        FOUR_DOOR_SUV  67520.0   \n",
       "1288                 NaN                  NaN      NaN   \n",
       "1289                 NaN                  NaN      NaN   \n",
       "1290            4 Series                COUPE  42500.0   \n",
       "1291                Soul  FOUR_DOOR_HATCHBACK  17700.0   \n",
       "\n",
       "                                                 extras avg_daily_price  \\\n",
       "0     AIR_MATTRESS,CHILD_SAFETY_SEAT,PET_FEE,CHILD_S...            60.0   \n",
       "1                           AIR_MATTRESS,PREPAID_REFUEL            89.0   \n",
       "2     AIR_MATTRESS,SLEEPING_BAG,UNLIMITED_MILES,PREP...           117.0   \n",
       "3           BEACH_GEAR,BEACH_GEAR,BEACH_GEAR,BEACH_GEAR            59.0   \n",
       "4     BEACH_GEAR,CHILD_SAFETY_SEAT,CHILD_SAFETY_SEAT...            65.0   \n",
       "...                                                 ...             ...   \n",
       "1287  UNLIMITED_MILES,STROLLER,PREPAID_REFUEL,CAMPIN...            91.0   \n",
       "1288                    UNLIMITED_MILES,UNLIMITED_MILES            52.0   \n",
       "1289  UNLIMITED_MILES,UNLIMITED_MILES,CHILD_SAFETY_S...            69.0   \n",
       "1290       UNLIMITED_MILES,UNLIMITED_MILES,ONE_WAY_TRIP            62.0   \n",
       "1291  UNLIMITED_MILES,UNLIMITED_MILES,UNLIMITED_MILE...            38.0   \n",
       "\n",
       "         list_countries                                        list_cities  \\\n",
       "0                 US,US                            El Monte,South El Monte   \n",
       "1                    US                                        Los Angeles   \n",
       "2           US,US,US,US                        Denver,Denver,Aurora,Aurora   \n",
       "3                    US                                             Kailua   \n",
       "4                    US                                            Kaneohe   \n",
       "...                 ...                                                ...   \n",
       "1287  US,US,US,US,US,US              Erie,Denver,Erie,Denver,Denver,Denver   \n",
       "1288              CA,CA                                  Montréal,Montréal   \n",
       "1289                 US                                      Grand Prairie   \n",
       "1290        US,US,US,US  Myrtle Beach,Sandy Springs,Sandy Springs,Atlanta    \n",
       "1291                 US                                          Baltimore   \n",
       "\n",
       "                                         list_latitudes  \\\n",
       "0                                 34.0869000,34.0532200   \n",
       "1                                            34.1812300   \n",
       "2           39.8329800,39.8213600,39.8198100,39.6833800   \n",
       "3                                            21.3873200   \n",
       "4                                            21.3924400   \n",
       "...                                                 ...   \n",
       "1287  40.0640500,39.8563600,40.0359200,39.8262900,39...   \n",
       "1288                              45.5657500,45.5226100   \n",
       "1289                                         32.7209900   \n",
       "1290        33.6822000,33.9941100,33.9940900,33.9946400   \n",
       "1291                                         39.3420800   \n",
       "\n",
       "                                        list_longitudes  \n",
       "0                             -118.0196000,-118.0655100  \n",
       "1                                          -118.4480300  \n",
       "2     -104.7047300,-104.7732900,-104.6877000,-104.81...  \n",
       "3                                          -157.7293900  \n",
       "4                                          -157.7970200  \n",
       "...                                                 ...  \n",
       "1287  -105.0611100,-104.6764100,-105.0545600,-104.76...  \n",
       "1288                            -73.6469600,-73.5857300  \n",
       "1289                                        -97.0444800  \n",
       "1290    -78.9278900,-84.3411200,-84.3411300,-84.3501600  \n",
       "1291                                        -76.5464900  \n",
       "\n",
       "[1292 rows x 23 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars = df_cars[df_cars.extras.isna() == False]\n",
    "df_golf = df_cars[df_cars.extras.str.contains('GOLF',case=False)]\n",
    "#df_cars = pd.concat([df_cars[:100], df_golf]).drop_duplicates()\n",
    "df_cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "623dcbef-057b-4ff0-8127-e1958d7cdb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'vehicle_id',\n",
       " 'color',\n",
       " 'automatic',\n",
       " 'odometer_range',\n",
       " 'market_country',\n",
       " 'number_of_doors',\n",
       " 'number_of_seats',\n",
       " 'fuel_type',\n",
       " 'make',\n",
       " 'description',\n",
       " 'vehicle_type',\n",
       " 'horsepower',\n",
       " 'year',\n",
       " 'model',\n",
       " 'body_type',\n",
       " 'msrp',\n",
       " 'extras',\n",
       " 'avg_daily_price',\n",
       " 'list_countries',\n",
       " 'list_cities',\n",
       " 'list_latitudes',\n",
       " 'list_longitudes']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cars.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "637683eb-e62d-4dac-a865-0f4a3594201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from zephyr-7b-beta.Q4_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:            blk.2.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:            blk.2.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:              blk.2.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:              blk.2.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:         blk.2.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:              blk.2.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:              blk.2.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:            blk.3.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:              blk.3.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:              blk.3.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:         blk.3.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:              blk.3.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:              blk.3.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:            blk.4.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:            blk.4.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:              blk.4.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:              blk.4.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:         blk.4.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:              blk.4.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:              blk.4.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:            blk.5.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:            blk.5.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:              blk.5.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:              blk.5.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:         blk.5.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:              blk.5.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:              blk.5.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:            blk.6.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:            blk.6.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:              blk.6.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:              blk.6.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:         blk.6.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:              blk.6.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:              blk.6.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:            blk.7.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:            blk.7.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:              blk.7.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:              blk.7.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:         blk.7.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:              blk.7.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:              blk.7.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:              blk.8.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:         blk.8.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:              blk.8.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:           blk.10.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:           blk.10.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.10.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:             blk.10.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:        blk.10.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:             blk.10.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.10.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:           blk.11.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:           blk.11.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.11.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:             blk.11.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:        blk.11.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:             blk.11.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.11.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.12.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.12.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:             blk.12.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:        blk.12.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.12.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:             blk.12.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:            blk.8.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:            blk.8.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:              blk.8.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:            blk.9.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:            blk.9.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:              blk.9.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:              blk.9.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:         blk.9.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:              blk.9.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.9.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.13.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.13.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.13.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.13.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.13.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.13.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.13.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.14.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.14.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.14.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.14.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.14.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.14.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.14.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.15.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.15.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.15.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.15.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.15.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.15.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.15.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.16.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.16.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.16.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.16.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.16.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.16.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.16.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:             blk.17.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:        blk.17.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:             blk.17.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:           blk.17.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:             blk.17.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:           blk.18.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:           blk.18.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:             blk.18.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:             blk.18.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:        blk.18.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:             blk.18.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:             blk.18.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:           blk.19.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:           blk.19.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:             blk.19.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:             blk.19.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:        blk.19.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:             blk.19.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:             blk.19.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:           blk.20.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:           blk.20.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:             blk.20.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:             blk.20.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:        blk.20.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:             blk.20.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:             blk.20.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.21.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:             blk.21.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:             blk.21.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:        blk.21.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:             blk.21.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:             blk.21.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:           blk.22.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:           blk.22.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:             blk.22.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:             blk.22.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:        blk.22.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:             blk.22.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:             blk.22.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:           blk.23.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:           blk.23.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:             blk.23.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:             blk.23.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:        blk.23.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:             blk.23.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:             blk.23.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:           blk.24.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:             blk.24.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:             blk.24.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:        blk.24.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:             blk.24.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:           blk.25.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:             blk.25.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:             blk.25.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:        blk.25.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:             blk.25.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.26.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:        blk.26.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:             blk.26.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:             blk.26.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:           blk.27.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:             blk.27.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:             blk.27.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:        blk.27.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:             blk.27.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:           blk.28.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:             blk.28.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:             blk.28.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:        blk.28.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:             blk.28.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:           blk.29.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:             blk.29.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:             blk.29.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:        blk.29.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:             blk.29.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:           blk.30.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:             blk.30.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:             blk.30.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:        blk.30.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:             blk.30.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:           blk.30.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q4_0     [ 14336,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_0     [  4096, 14336,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_0     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q4_0     [  4096,  1024,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = huggingfaceh4_zephyr-7b-beta\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_0\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 3.83 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = huggingfaceh4_zephyr-7b-beta\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 2 '</s>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 3917.97 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 5.50 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "#filename = \"zephyr-7b-beta.Q4_0.gguf\"\n",
    "filename = \"zephyr-7b-beta.Q4_0.gguf\"\n",
    "\n",
    "model_path = expanduser(filename)\n",
    "\n",
    "llama = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    streaming=False,\n",
    "    n_ctx=2048\n",
    ")\n",
    "model = Llama2Chat(llm=llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b64805-621f-498e-8f8c-bce5cc8da37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292it [00:00, 10470.34it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This vehicle is a nan SUV from the make SUBARU. The transmission is [1].    It has a capacity of 5.0 passengers and has 5.0 doors. The motor runs with HYBRID and a power of 148.0 horses. It has been built in 2022.0    and is about 0-10 thousands kilometers. It has a FOUR_DOOR_SUV shape.    The car worth 36345.0 dollars and is available at a daily price of 60.0 dollars. It is available in El Monte\\', \\'South El Monte.    Extra of the cars are AIR_MATTRESS, CHILD_SAFETY_SEAT, PET_FEE, CHILD_SAFETY_SEAT, UNLIMITED_MILES. The owner describes it as \"Our services:✈️**LAX guests**✈️*FREE* *LOWER FEES* shuttle from LAX to our meet up parking lot. Shuttle comes every 7-9 mins right from your arrival terminal. Please select custom delivery location for 11500 Aviation Blvd, Los Angeles, CA 90045Yes! It\\'s a HYBRID All-Wheel-Drive!Up to 90MPGe (gas&electric) Brand new 2022 Subaru Crosstrek Hybrid Limited!  With gas prices now days, this is Perfect for road trips to campsites in any road conditions! Tip: Plug in and charge at your camp site and get more mpg to your gas tank!Features:Heated seatsKeyless entryAndroid autoApple car playLeather interiorPlug in wall chargerRemote a/c starterX-mode (use when needed in different road conditions)Big dual screen infotainment Adaptive cruise controlEye sight lane assist Semi auto pilotAnd many more options!Please return vehicle in a similar interior and exterior condition as it was rented - does not have to be perfect, just not excessively messy. Thank you!*****If you incurred tolls during your trip you will be invoice after the trip end.***.\"   '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install tqdm -q\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from tqdm import tqdm\n",
    "\n",
    "list_descriptions = []\n",
    "\n",
    "for index, row in tqdm(df_golf.iterrows()):\n",
    "    \n",
    "   text_description =  f\"\"\"This vehicle is a {row[\"color\"]} {row[\"vehicle_type\"]} from the make {row[\"make\"]}. The transmission is {[row[\"automatic\"]]}. \n",
    "   It has a capacity of {row[\"number_of_seats\"]} passengers and has {row[\"number_of_doors\"]} doors. The motor runs with {row[\"fuel_type\"]} and a power of {row[\"horsepower\"]} horses. It has been built in {row[\"year\"]} \n",
    "   and is about {row[\"odometer_range\"]} thousands kilometers. It has a {(row[\"body_type\"])} shape. \n",
    "   The car worth {row[\"msrp\"]} dollars and is available at a daily price of {row[\"avg_daily_price\"]} dollars. It is available in {str(set(row[\"list_cities\"].split(\",\")))[2:-2]}. \n",
    "   Extra of the cars are {row[\"extras\"].replace(',', ', ')}. The owner describes it as \"{row[\"description\"]}.\"\n",
    "   \"\"\".replace('\\n', '').replace('\\r', '')\n",
    "   list_descriptions.append(text_description)\n",
    "\n",
    "print(list_descriptions[0])\n",
    "\n",
    "def generate_text(\n",
    "    prompt=\"Who is the CEO of Apple?\",\n",
    "    max_tokens=256,\n",
    "    temperature=0.9,\n",
    "    top_p=0.5,\n",
    "    echo=False,              # Echo the prompt back in the output\n",
    "    stop=[\"#\"],  # Stop generating just before the model would generate a new question\n",
    "):\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    #output_text = output[\"choices\"][0][\"text\"].strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_prompt_from_template(input):\n",
    "    input = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant. You act as a car seller. You can't lie or say something false. Don't include phone number. Create a more textual description based on the following details of the car: \" + str(input) + \". Don't tell anything about the owner's phone number or any private information. Limit your response at 100 words in an only paragraphe.\"\n",
    "    chat_prompt_template = f\"\"\"User: {input}\n",
    "                               Assistant:\"\"\"\n",
    "    return chat_prompt_template\n",
    "\n",
    "\n",
    "doc =  Document(\n",
    "            page_content=list_descriptions[0],\n",
    "            metadata=metadata,\n",
    "        )\n",
    "\n",
    "text = doc.page_content\n",
    "\n",
    "prompt = generate_prompt_from_template(text)\n",
    "\n",
    "print('prompt:', prompt)\n",
    "\n",
    "for i in range(5):\n",
    "    result = generate_text(\n",
    "        prompt,\n",
    "        #max_tokens=100,\n",
    "    )\n",
    "    print(result)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f87766-6aba-44b5-ad71-b118412a4a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents:  1292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Our services:\\n✈️**LAX guests**✈️\\n*FREE* *LOWER FEES* shuttle from LAX to our meet up parking lot. Shuttle comes every 7-9 mins right from your arrival terminal. Please select custom delivery location for \\n11500 Aviation Blvd, Los Angeles, CA 90045\\n\\n\\n\\nYes! It's a HYBRID All-Wheel-Drive!\\nUp to 90MPGe (gas&electric) \\nBrand new 2022 Subaru Crosstrek Hybrid Limited!  \\n\\nWith gas prices now days, this is Perfect for road trips to campsites in any road conditions! \\nTip: Plug in and charge at your camp site and get more mpg to your gas tank!\\n\\nFeatures:\\nHeated seats\\nKeyless entry\\nAndroid auto\\nApple car play\\nLeather interior\\nPlug in wall charger\\nRemote a/c starter\\nX-mode (use when needed in different road conditions)\\nBig dual screen infotainment \\nAdaptive cruise control\\nEye sight lane assist \\nSemi auto pilot\\nAnd many more options!\\n\\nPlease return vehicle in a similar interior and exterior condition as it was rented - does not have to be perfect, just not excessively messy. Thank you!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*****If you incurred tolls during your trip you will be invoice after the trip end.***  The car contains the optional extra air_mattress. The car contains the optional extra child_safety_seat. The car contains the optional extra pet_fee. The car contains the optional extra child_safety_seat. The car contains the optional extra unlimited_miles. The car is available in the country us. The car is available in the country us. The car is available in the city el monte. The car is available in the city south el monte. The car is located at the coordinates (34.0869000;-118.0196000). The car is located at the coordinates (34.0532200;-118.0655100).\", metadata={'id': 1384926, 'vehicle_id': 1384926, 'color': nan, 'automatic': 1, 'odometer_range': '0-10', 'number_of_doors': 5.0, 'number_of_seats': 5.0, 'fuel_type': 'hybrid', 'make': 'subaru', 'description': \"our services:\\n✈️**lax guests**✈️\\n*free* *lower fees* shuttle from lax to our meet up parking lot. shuttle comes every 7-9 mins right from your arrival terminal. please select custom delivery location for \\n11500 aviation blvd, los angeles, ca 90045\\n\\n\\n\\nyes! it's a hybrid all-wheel-drive!\\nup to 90mpge (gas&electric) \\nbrand new 2022 subaru crosstrek hybrid limited!  \\n\\nwith gas prices now days, this is perfect for road trips to campsites in any road conditions! \\ntip: plug in and charge at your camp site and get more mpg to your gas tank!\\n\\nfeatures:\\nheated seats\\nkeyless entry\\nandroid auto\\napple car play\\nleather interior\\nplug in wall charger\\nremote a/c starter\\nx-mode (use when needed in different road conditions)\\nbig dual screen infotainment \\nadaptive cruise control\\neye sight lane assist \\nsemi auto pilot\\nand many more options!\\n\\nplease return vehicle in a similar interior and exterior condition as it was rented - does not have to be perfect, just not excessively messy. thank you!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n*****if you incurred tolls during your trip you will be invoice after the trip end.***\", 'vehicle_type': 'suv', 'horsepower': 148.0, 'year': 2022.0, 'model': 'crosstrek', 'body_type': 'four_door_suv', 'msrp': 36345.0, 'avg_daily_price': 60.0, 'market_country': 'us', 'list_countries': 'us,us', 'list_cities': 'el monte,south el monte', 'list_latitudes': '34.0869000,34.0532200', 'list_longitudes': '-118.0196000,-118.0655100', 'extras': 'air_mattress,child_safety_seat,pet_fee,child_safety_seat,unlimited_miles'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Metadata to existing documents \n",
    "meta_cols = ['id',\n",
    "'vehicle_id',\n",
    "'color',\n",
    "'automatic',\n",
    "'odometer_range',\n",
    "'number_of_doors',\n",
    "'number_of_seats',\n",
    "'fuel_type',\n",
    "'make',\n",
    "'description',\n",
    "'vehicle_type',\n",
    "'horsepower',\n",
    "'year',\n",
    "'model',\n",
    "'body_type',\n",
    "'msrp',\n",
    "'avg_daily_price',\n",
    "'market_country',\n",
    "'list_countries',\n",
    "'list_cities',\n",
    "'list_latitudes',\n",
    "'list_longitudes',\n",
    "'extras',\n",
    "'description']\n",
    "\n",
    "docs = []\n",
    "for index, row in df_cars.iterrows():\n",
    "\n",
    "    #Add infos in dthe description of the host \n",
    "    description = str(row[\"description\"])\n",
    "    description += ' '\n",
    "    \n",
    "    #Add extras\n",
    "    extras = row['extras'].split(\",\")\n",
    "    for i, extra in enumerate(extras):\n",
    "        description += \" The car contains the optional extra \" + extra.lower() + \".\"\n",
    "\n",
    "    #Add Countries\n",
    "    countries = row['list_countries'].split(\",\")\n",
    "    for i, country in enumerate(countries):\n",
    "        description += \" The car is available in the country \" + country.lower() + \".\"\n",
    "\n",
    "    #Add Cities\n",
    "    cities = row['list_cities'].split(\",\")\n",
    "    for i, city in enumerate(cities):\n",
    "        description += \" The car is available in the city \" + city.lower() + \".\"\n",
    "\n",
    "    #Add Lat;Lng\n",
    "    lats = row['list_latitudes'].split(\",\")\n",
    "    lngs = row['list_longitudes'].split(\",\")\n",
    "\n",
    "    for i, lat in enumerate(lats):\n",
    "        description += \" The car is located at the coordinates (\" + str(lats[i]) + ';' + str(lngs[i]) + ')' + \".\"\n",
    "        \n",
    "    #Split the description \n",
    "    #text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=50, add_start_index=True)\n",
    "    #list_desc = text_splitter.split_text(description)\n",
    "    list_desc = [description]\n",
    "    \n",
    "    for d, desc in enumerate(list_desc):\n",
    "        metadata = {}\n",
    "        for col in meta_cols:\n",
    "            if type(row[col]) == str:\n",
    "                metadata[col] = row[col].lower()\n",
    "            else:\n",
    "                metadata[col] = row[col]\n",
    "\n",
    "        doc =  Document(\n",
    "            page_content=desc,\n",
    "            metadata=metadata,\n",
    "        )\n",
    "        \n",
    "        docs.append(doc)\n",
    "print(\"Total documents: \", len(docs))\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2917c88b-9573-484f-af47-08bfc66e2f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sentence_transformers.SentenceTransformer:No sentence-transformers model found with name /Users/nicolas/.cache/torch/sentence_transformers/bert-base-uncased. Creating a new one with MEAN pooling.\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48337271-288e-419f-bf9b-97eebc895f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05353f13-b0bc-4c37-9fb9-f3b06e986372",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_api_key = \"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\"\n",
    "hugging_face_key = \"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\"\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hugging_face_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = open_api_key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "635a0224-31e0-45c1-93e4-dd5643209bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5a3ccc1-1235-47f0-91f9-465312cb46fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"color\",\n",
    "        description=\"The color of the car\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"automatic\",\n",
    "        description=\"true if the gearbox of the car is automatic, else is manual\",\n",
    "        type=\"boolean\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"odometer_range\",\n",
    "        description=\"Number of miles at the odometer in thousands with the format min-max\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "#    AttributeInfo(\n",
    "#        name=\"market_country\", description=\"Country the car is located\", type=\"string\"\n",
    "#    ),\n",
    "    AttributeInfo(\n",
    "        name=\"number_of_doors\", description=\"Number of doors of the car\", type=\"float\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"number_of_seats\", description=\"number of maximum passengers for the car\", type=\"float\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"fuel_type\", description=\"The engine type of the car\", type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"make\", description=\"The car brand in lower case\", type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"vehicle_type\", description=\"the vehicle category type, is one of [nan, 'CAR', 'SUV', 'TRUCK']\", type=\"string\"\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"horsepower\",\n",
    "        description=\"Total number of horsepower of the car\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"year\",\n",
    "        description=\"Year of production of the car\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"model\",\n",
    "        description=\"Model name of the car in lower case\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"body_type\",\n",
    "        description=\"Body type name of the car. Similar to category of the car.\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"msrp\",\n",
    "        description=\"Value of the car in dollars\",\n",
    "        type=\"float\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"avg_daily_price\",\n",
    "        description=\"Daily price for the car rental in dollars\",\n",
    "        type=\"float\",\n",
    "    )\n",
    "#    AttributeInfo(\n",
    "#        name=\"list_countries\",\n",
    "#        description=\"List of the countries where the car is available. Split by a comma between countries. Should be a country and not a region\",\n",
    "#        type=\"string\",\n",
    "#    ),\n",
    "#    AttributeInfo(\n",
    "#        name=\"list_cities\",\n",
    "#        description=\"List of the citites where the car is available. Split by a comma between cities.\",\n",
    "#        type=\"string\",\n",
    "#    ),\n",
    "#    AttributeInfo(\n",
    "#        name=\"list_latitudes\",\n",
    "#        description=\"List of the latitudes where the car is available. Split by a comma between latitudes.\",\n",
    "#        type=\"string\",\n",
    "#    ),\n",
    "#    AttributeInfo(\n",
    "#        name=\"list_longitudes\",\n",
    "#        description=\"List of the longitudes where the car is available. Split by a comma between longitudes.\",\n",
    "#        type=\"string\",\n",
    "#    ),\n",
    "#    AttributeInfo(\n",
    "#        name=\"extras\",\n",
    "#        description=\"List of available options for the car\",\n",
    "#        type=\"string\",\n",
    "#    ),\n",
    "    \n",
    "]\n",
    "\n",
    "document_content_description = \"Summary of a rental car\"\n",
    "\n",
    "#retriever = SelfQueryRetriever.from_llm(\n",
    "#    llm,\n",
    "#    vectorstore,\n",
    "#    document_content_description,\n",
    "#    metadata_field_info,\n",
    "#    use_original_query=True,\n",
    "#    verbose=True\n",
    "#)\n",
    "\n",
    "\n",
    "chain = load_query_constructor_runnable(\n",
    "    llm,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    #examples=examples,\n",
    "    fix_invalid=True,\n",
    "    allowed_comparators=['like', 'ne', 'gt', 'gte', 'lt', 'lte'],\n",
    "    allowed_operators=['and', 'or', 'not']\n",
    ")\n",
    "\n",
    "retriever = SelfQueryRetriever(\n",
    "    query_constructor=chain, vectorstore=vectorstore, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc780317-a85d-4dd2-b4e5-290feacbb9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an AI language model assistant. Your task is to retrieve relevant documents of the given user request. \n",
      "    Original request: I want to go in California next year with my 2 children and my wife. I would like american SUV. And I’m a big fan of golf so I plan to play 2-3 times (don’t tell my wife lol) but I won’t have my clubs. The car must have a golf bag included\n"
     ]
    }
   ],
   "source": [
    "# This example only specifies a filter\n",
    "question = \" I want a car to go on a roadtrip in the mountains to go on a ski trip. I need it to be safe regarding the snow on the road. Also, a panoramic roof would be cool\"\n",
    "question3 = \"Find a car corresponding to this description: I want a TOYOTA SUV produced after 2014 to go on a roadtrip in California during Christmas with my wife and my 2 children. The car must have a golf bag included.\"\n",
    "request = \"I want to go in California next year with my 2 children and my wife. I would like american SUV. And I’m a big fan of golf so I plan to play 2-3 times (don’t tell my wife lol) but I won’t have my clubs. The car must have a golf bag included\"\n",
    "\n",
    "template=\"\"\"You are an AI language model assistant. Your task is to retrieve relevant documents of the given user request. \n",
    "    Original request: \"\"\" + str(request)\n",
    "print(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58348cfc-46b7-4ad3-91ab-b12f40576a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"query\": \"golf bag\",\n",
      "    \"filter\": \"and(eq(\\\"vehicle_type\\\", \\\"SUV\\\"), eq(\\\"make\\\", \\\"american\\\"), gte(\\\"number_of_seats\\\", 4))\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "results = retriever.get_relevant_documents(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4748c32-1d1b-4440-b99a-15703cfa99c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents found: 4\n",
      "\n",
      "Get ready to zip around the island on this 2013 Audi A5 convertible.  She is so fun to drive, attacking every corner like a surfer on a wave.  The back seat is larger than most convertibles and fits 2 people comfortably. One button puts the top up or down as well as one button up and down for the windows. She is easy to park anywhere on the island in any size space, perfect for the island.  Let this be your fun in the sun for your trip to Oahu.\n",
      "\n",
      "***Please pack accordingly. The truck can hold 2-3 carry on suitcases and backpacks. A large suitcase will not fit in the trunk and will damage the tops storage box. ***  The car contains the optional extra surfboard. The car contains the optional extra prepaid_refuel. The car contains the optional extra beach_gear. The car contains the optional extra beach_gear. The car contains the optional extra portable_speaker. The car is available in the country us. The car is available in the city honolulu. The car is located at the coordinates (21.3400000;-157.9209300).\n",
      "{'automatic': 1, 'avg_daily_price': 74.0, 'color': 'black', 'description': 'get ready to zip around the island on this 2013 audi a5 convertible.  she is so fun to drive, attacking every corner like a surfer on a wave.  the back seat is larger than most convertibles and fits 2 people comfortably. one button puts the top up or down as well as one button up and down for the windows. she is easy to park anywhere on the island in any size space, perfect for the island.  let this be your fun in the sun for your trip to oahu.\\r\\n\\r\\n***please pack accordingly. the truck can hold 2-3 carry on suitcases and backpacks. a large suitcase will not fit in the trunk and will damage the tops storage box. ***', 'extras': 'surfboard,prepaid_refuel,beach_gear,beach_gear,portable_speaker', 'fuel_type': 'gasoline', 'id': 1344737, 'list_cities': 'honolulu', 'list_countries': 'us', 'list_latitudes': '21.3400000', 'list_longitudes': '-157.9209300', 'market_country': 'us', 'number_of_doors': 2.0, 'number_of_seats': 4.0, 'odometer_range': '40-50', 'vehicle_id': 1344737}\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents found:\", len(results))\n",
    "print()\n",
    "\n",
    "print(results[0].page_content)\n",
    "print(results[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95b9d4c7-6b27-40cb-b439-5b801f465e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['.', '', 'What is the best American SUV for a family of four to take a trip to California next year?', 'What is the best American SUV for a family of four to take a vacation to California in 2021?', 'What is the best American SUV for a family of four to travel to California in 2021 with a golf bag included?', 'What is the best American SUV for a family of four to go to California in 2021 with a golf bag included?', 'What is the best American SUV for a family of four to go to California in 2021 with a golf bag and the ability to play golf?']\n"
     ]
    }
   ],
   "source": [
    "### OWN PROMPT FOR GENERATE QUERIES \n",
    "# Output parser will split the LLM result into a list of queries\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from typing import List\n",
    "from langchain.prompts import PromptTemplate\n",
    "import logging\n",
    "\n",
    "\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "    different versions of the given user question to retrieve relevant documents from a vector \n",
    "    database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    the user overcome some of the limitations of the distance-based similarity search. \n",
    "    Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=vectorstore.as_retriever(), \n",
    "    llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.DEBUG)\n",
    "\n",
    "# Results\n",
    "results = retriever.get_relevant_documents(\n",
    "    query=request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05c21cc5-ba06-4670-9573-96c7bd106fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents found: 19\n",
      "\n",
      "Hello and thank you for considering our 2021 Nissan Kicks. It’s a small SUV with lots of room and plenty of punch. It’s great for getting around the city. Great on fuel. It’s sporty and fun. We hope it makes your vacation or business trip more enjoyable.  The car contains the optional extra golf_clubs. The car contains the optional extra cooler. The car contains the optional extra camp_chair. The car contains the optional extra stroller. The car contains the optional extra prepaid_refuel. The car contains the optional extra unlimited_miles. The car contains the optional extra child_safety_seat. The car is available in the country us. The car is available in the country us. The car is available in the city harahan. The car is available in the city river ridge. The car is located at the coordinates (29.9397500;-90.1940100). The car is located at the coordinates (29.9578000;-90.2173100).\n",
      "{'automatic': 1, 'avg_daily_price': 60.0, 'body_type': 'four_door_suv', 'color': 'gray', 'description': 'hello and thank you for considering our 2021 nissan kicks. it’s a small suv with lots of room and plenty of punch. it’s great for getting around the city. great on fuel. it’s sporty and fun. we hope it makes your vacation or business trip more enjoyable.', 'extras': 'golf_clubs,cooler,camp_chair,stroller,prepaid_refuel,unlimited_miles,child_safety_seat', 'fuel_type': 'gasoline', 'horsepower': 122.0, 'id': 1329642, 'list_cities': 'harahan,river ridge', 'list_countries': 'us,us', 'list_latitudes': '29.9397500,29.9578000', 'list_longitudes': '-90.1940100,-90.2173100', 'make': 'nissan', 'market_country': 'us', 'model': 'kicks', 'msrp': 22140.0, 'number_of_doors': 4.0, 'number_of_seats': 5.0, 'odometer_range': '0-10', 'vehicle_id': 1329642, 'vehicle_type': 'suv', 'year': 2021.0}\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Our VW Passat 🚗 , recently named Black Pearl, rides extremely smooth. It’s spacious, has modern amenities, blows hot 🔥 when necessary and cold 🥶 when required. The Passat is also great on gas. \n",
      "\n",
      "Our goal is to make this experience as simple and smooth as possible for you. Let us know what you need and we’ll be there for you! ⭐️⭐️⭐️⭐️⭐️\n",
      "\n",
      "-Delivery & Pick Up available $40 total\n",
      "-Prepaid gas ⛽️ $60\n",
      "-Car Seat $25/each\n",
      "-Stollers $20/each\n",
      "-Pets 🐶 welcome  The car contains the optional extra prepaid_refuel. The car contains the optional extra child_safety_seat. The car contains the optional extra cooler. The car contains the optional extra stroller. The car is available in the country us. The car is available in the country us. The car is available in the city austin. The car is available in the city austin. The car is located at the coordinates (30.3409600;-97.7045500). The car is located at the coordinates (30.1974700;-97.6663500).\n",
      "{'automatic': 1, 'avg_daily_price': 50.0, 'color': 'black', 'description': 'our vw passat 🚗 , recently named black pearl, rides extremely smooth. it’s spacious, has modern amenities, blows hot 🔥 when necessary and cold 🥶 when required. the passat is also great on gas. \\n\\nour goal is to make this experience as simple and smooth as possible for you. let us know what you need and we’ll be there for you! ⭐️⭐️⭐️⭐️⭐️\\n\\n-delivery & pick up available $40 total\\n-prepaid gas ⛽️ $60\\n-car seat $25/each\\n-stollers $20/each\\n-pets 🐶 welcome', 'extras': 'prepaid_refuel,child_safety_seat,cooler,stroller', 'fuel_type': 'gasoline', 'id': 1335706, 'list_cities': 'austin,austin', 'list_countries': 'us,us', 'list_latitudes': '30.3409600,30.1974700', 'list_longitudes': '-97.7045500,-97.6663500', 'market_country': 'us', 'number_of_doors': 4.0, 'number_of_seats': 5.0, 'odometer_range': '80-90', 'vehicle_id': 1335706}\n",
      "\n",
      "----------------------------\n",
      "\n",
      "I’ve driven a KIA Van in the early 2000’s shuttling around pre-teens and taking many trips to sporting tournaments throughout California. It was always a deluxe ride with all the room & comfortable for parents,kids, & all the gear & luggage we needed!!\n",
      "A smooth ride all the way🤙\n",
      "Leather interior, tinted windows, and great sound system for cranking up favorite tubes.\n",
      "Getting in & out of the van is a comfortable low profile for the young and elderly.  The car contains the optional extra child_safety_seat. The car is available in the country us. The car is available in the country us. The car is available in the city lihue. The car is available in the city kalaheo . The car is located at the coordinates (21.9788500;-159.3437600). The car is located at the coordinates (21.9063000;-159.5189700).\n",
      "{'automatic': 1, 'avg_daily_price': 104.0, 'body_type': 'passenger_minivan', 'description': 'i’ve driven a kia van in the early 2000’s shuttling around pre-teens and taking many trips to sporting tournaments throughout california. it was always a deluxe ride with all the room & comfortable for parents,kids, & all the gear & luggage we needed!!\\na smooth ride all the way🤙\\nleather interior, tinted windows, and great sound system for cranking up favorite tubes.\\ngetting in & out of the van is a comfortable low profile for the young and elderly.', 'extras': 'child_safety_seat', 'fuel_type': 'gasoline', 'horsepower': 276.0, 'id': 1373890, 'list_cities': 'lihue,kalaheo ', 'list_countries': 'us,us', 'list_latitudes': '21.9788500,21.9063000', 'list_longitudes': '-159.3437600,-159.5189700', 'make': 'kia', 'market_country': 'us', 'model': 'sedona', 'msrp': 36300.0, 'number_of_doors': 4.0, 'number_of_seats': 7.0, 'odometer_range': '100-110', 'vehicle_id': 1373890, 'vehicle_type': 'minivan', 'year': 2015.0}\n",
      "\n",
      "----------------------------\n",
      "\n",
      "Tour the Charleston area in the most fuel-efficient hybrid vehicle on the market…(the world!!). This 2022 Prius lives up to its Latin meaning of first, superior, and original. No other compact car on the market will provide a similar experience. You can rest assured that its five-star NHTSA rating will keep you and your passengers safe during your journey in the low country. With gas prices over $3 a gallon you’ll be happy you are driving a vehicle that gets over 50 miles per gallon and actually gets better mileage in city driving. They call it the more convenient version of the Tesla (perhaps better in many ways).\n",
      "\n",
      "- off roading in this vehicle is prohibited!  The car contains the optional extra cooler. The car contains the optional extra child_safety_seat. The car contains the optional extra cooler. The car contains the optional extra unlimited_miles. The car contains the optional extra surfboard. The car contains the optional extra beach_gear. The car is available in the country us. The car is available in the country us. The car is available in the city charleston. The car is available in the city charleston. The car is located at the coordinates (32.7003300;-79.9672000). The car is located at the coordinates (32.7899700;-79.9597700).\n",
      "{'automatic': 1, 'avg_daily_price': 63.0, 'body_type': 'four_door_hatchback', 'description': 'tour the charleston area in the most fuel-efficient hybrid vehicle on the market…(the world!!). this 2022 prius lives up to its latin meaning of first, superior, and original. no other compact car on the market will provide a similar experience. you can rest assured that its five-star nhtsa rating will keep you and your passengers safe during your journey in the low country. with gas prices over $3 a gallon you’ll be happy you are driving a vehicle that gets over 50 miles per gallon and actually gets better mileage in city driving. they call it the more convenient version of the tesla (perhaps better in many ways).\\n\\n- off roading in this vehicle is prohibited!', 'extras': 'cooler,child_safety_seat,cooler,unlimited_miles,surfboard,beach_gear', 'fuel_type': 'hybrid', 'horsepower': 121.0, 'id': 1329060, 'list_cities': 'charleston,charleston', 'list_countries': 'us,us', 'list_latitudes': '32.7003300,32.7899700', 'list_longitudes': '-79.9672000,-79.9597700', 'make': 'toyota', 'market_country': 'us', 'model': 'prius', 'msrp': 33370.0, 'number_of_doors': 4.0, 'number_of_seats': 5.0, 'odometer_range': '0-10', 'vehicle_id': 1329060, 'vehicle_type': 'car', 'year': 2022.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Total documents found:\", len(results))\n",
    "print()\n",
    "\n",
    "print(results[0].page_content)\n",
    "print(results[0].metadata)\n",
    "\n",
    "print()\n",
    "print('----------------------------')\n",
    "print()\n",
    "\n",
    "print(results[1].page_content)\n",
    "print(results[1].metadata)\n",
    "\n",
    "print()\n",
    "print('----------------------------')\n",
    "print()\n",
    "\n",
    "print(results[2].page_content)\n",
    "print(results[2].metadata)\n",
    "\n",
    "print()\n",
    "print('----------------------------')\n",
    "print()\n",
    "\n",
    "print(results[3].page_content)\n",
    "print(results[3].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cdefc8-2c6f-4185-b33e-be7a8fdf1851",
   "metadata": {},
   "source": [
    "# Generate story telling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "145e4e31-8446-4a78-903b-07d87ca75df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from codellama-7b-python.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: - tensor    0:                token_embd.weight q4_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor    1:           blk.0.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    2:            blk.0.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    3:            blk.0.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    4:              blk.0.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor    5:            blk.0.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor    6:              blk.0.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    7:         blk.0.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    8:              blk.0.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor    9:              blk.0.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   10:           blk.1.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   11:            blk.1.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   12:            blk.1.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   13:              blk.1.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   14:            blk.1.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   15:              blk.1.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   16:         blk.1.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   17:              blk.1.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   18:              blk.1.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   19:          blk.10.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   20:           blk.10.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   21:           blk.10.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   22:             blk.10.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   23:           blk.10.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   24:             blk.10.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   25:        blk.10.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   26:             blk.10.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   27:             blk.10.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   28:          blk.11.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   29:           blk.11.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   30:           blk.11.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   31:             blk.11.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   32:           blk.11.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   33:             blk.11.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   34:        blk.11.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   35:             blk.11.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   36:             blk.11.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   37:          blk.12.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   38:           blk.12.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   39:           blk.12.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   40:             blk.12.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   41:           blk.12.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   42:             blk.12.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   43:        blk.12.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   44:             blk.12.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   45:             blk.12.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   46:          blk.13.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   47:           blk.13.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   48:           blk.13.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   49:             blk.13.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   50:           blk.13.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   51:             blk.13.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   52:        blk.13.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   53:             blk.13.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   54:             blk.13.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   55:          blk.14.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   56:           blk.14.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   57:           blk.14.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   58:             blk.14.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   59:           blk.14.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   60:             blk.14.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   61:        blk.14.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   62:             blk.14.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   63:             blk.14.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   64:          blk.15.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   65:           blk.15.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   66:           blk.15.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   67:             blk.15.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   68:           blk.15.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   69:             blk.15.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   70:        blk.15.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   71:             blk.15.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   72:             blk.15.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   73:          blk.16.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   74:           blk.16.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   75:           blk.16.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   76:             blk.16.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   77:           blk.16.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   78:             blk.16.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   79:        blk.16.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   80:             blk.16.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   81:             blk.16.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   82:          blk.17.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   83:           blk.17.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   84:           blk.17.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   85:             blk.17.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   86:           blk.17.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   87:             blk.17.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   88:        blk.17.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   89:             blk.17.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   90:             blk.17.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   91:          blk.18.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   92:           blk.18.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   93:           blk.18.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   94:             blk.18.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor   95:           blk.18.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor   96:             blk.18.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   97:        blk.18.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   98:             blk.18.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor   99:             blk.18.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  100:          blk.19.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  101:           blk.19.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  102:           blk.19.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  103:             blk.19.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  104:           blk.19.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  105:             blk.19.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  106:        blk.19.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  107:             blk.19.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  108:             blk.19.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  109:           blk.2.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  110:            blk.2.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  111:            blk.2.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  112:              blk.2.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  113:            blk.2.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  114:              blk.2.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  115:         blk.2.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  116:              blk.2.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  117:              blk.2.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  118:          blk.20.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  119:           blk.20.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  120:           blk.20.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  121:             blk.20.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  122:           blk.20.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  123:             blk.20.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  124:        blk.20.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  125:             blk.20.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  126:             blk.20.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  127:          blk.21.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  128:           blk.21.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  129:           blk.21.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  130:             blk.21.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  131:           blk.21.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  132:             blk.21.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  133:        blk.21.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  134:             blk.21.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  135:             blk.21.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  136:          blk.22.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  137:           blk.22.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  138:           blk.22.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  139:             blk.22.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  140:           blk.22.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  141:             blk.22.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  142:        blk.22.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  143:             blk.22.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  144:             blk.22.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  145:          blk.23.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  146:           blk.23.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  147:           blk.23.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  148:             blk.23.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  149:           blk.23.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  150:             blk.23.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  151:        blk.23.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  152:             blk.23.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  153:             blk.23.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  154:           blk.3.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  155:            blk.3.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  156:            blk.3.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  157:              blk.3.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  158:            blk.3.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  159:              blk.3.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  160:         blk.3.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  161:              blk.3.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  162:              blk.3.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  163:           blk.4.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  164:            blk.4.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  165:            blk.4.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  166:              blk.4.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  167:            blk.4.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  168:              blk.4.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  169:         blk.4.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  170:              blk.4.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  171:              blk.4.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  172:           blk.5.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  173:            blk.5.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  174:            blk.5.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  175:              blk.5.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  176:            blk.5.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  177:              blk.5.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  178:         blk.5.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  179:              blk.5.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  180:              blk.5.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  181:           blk.6.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  182:            blk.6.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  183:            blk.6.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  184:              blk.6.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  185:            blk.6.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  186:              blk.6.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  187:         blk.6.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  188:              blk.6.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  189:              blk.6.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  190:           blk.7.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  191:            blk.7.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  192:            blk.7.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  193:              blk.7.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  194:            blk.7.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  195:              blk.7.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  196:         blk.7.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  197:              blk.7.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  198:              blk.7.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  199:           blk.8.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  200:            blk.8.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  201:            blk.8.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  202:              blk.8.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  203:            blk.8.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  204:              blk.8.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  205:         blk.8.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  206:              blk.8.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  207:              blk.8.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  208:           blk.9.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  209:            blk.9.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  210:            blk.9.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  211:              blk.9.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  212:            blk.9.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  213:              blk.9.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  214:         blk.9.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  215:              blk.9.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  216:              blk.9.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  217:                    output.weight q6_K     [  4096, 32000,     1,     1 ]\n",
      "llama_model_loader: - tensor  218:          blk.24.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  219:           blk.24.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  220:           blk.24.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  221:             blk.24.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  222:           blk.24.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  223:             blk.24.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  224:        blk.24.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  225:             blk.24.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  226:             blk.24.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  227:          blk.25.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  228:           blk.25.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  229:           blk.25.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  230:             blk.25.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  231:           blk.25.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  232:             blk.25.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  233:        blk.25.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  234:             blk.25.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  235:             blk.25.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  236:          blk.26.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  237:           blk.26.ffn_down.weight q4_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  238:           blk.26.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  239:             blk.26.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  240:           blk.26.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  241:             blk.26.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  242:        blk.26.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  243:             blk.26.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  244:             blk.26.attn_v.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  245:          blk.27.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  246:           blk.27.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  247:           blk.27.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  248:             blk.27.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  249:           blk.27.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  250:             blk.27.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  251:        blk.27.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  252:             blk.27.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  253:             blk.27.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  254:          blk.28.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  255:           blk.28.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  256:           blk.28.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  257:             blk.28.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  258:           blk.28.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  259:             blk.28.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  260:        blk.28.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  261:             blk.28.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  262:             blk.28.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  263:          blk.29.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  264:           blk.29.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  265:           blk.29.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  266:             blk.29.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  267:           blk.29.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  268:             blk.29.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  269:        blk.29.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  270:             blk.29.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  271:             blk.29.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  272:          blk.30.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  273:           blk.30.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  274:           blk.30.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  275:             blk.30.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  276:           blk.30.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  277:             blk.30.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  278:        blk.30.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  279:             blk.30.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  280:             blk.30.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  281:          blk.31.attn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  282:           blk.31.ffn_down.weight q6_K     [ 11008,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  283:           blk.31.ffn_gate.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  284:             blk.31.ffn_up.weight q4_K     [  4096, 11008,     1,     1 ]\n",
      "llama_model_loader: - tensor  285:           blk.31.ffn_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: - tensor  286:             blk.31.attn_k.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  287:        blk.31.attn_output.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  288:             blk.31.attn_q.weight q4_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  289:             blk.31.attn_v.weight q6_K     [  4096,  4096,     1,     1 ]\n",
      "llama_model_loader: - tensor  290:               output_norm.weight f32      [  4096,     1,     1,     1 ]\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-7b-python-hf\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 16384\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 16384\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = mostly Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = codellama_codellama-7b-python-hf\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: mem required  = 3891.35 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 5.50 MiB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "#filename = \"zephyr-7b-beta.Q4_0.gguf\"\n",
    "filename = \"codellama-7b-python.Q4_K_M.gguf\"\n",
    "\n",
    "model_path = expanduser(filename)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    model_path=model_path,\n",
    "    streaming=False,\n",
    "    n_ctx=2048\n",
    ")\n",
    "model = Llama2Chat(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13823f94-d98d-485a-a41d-b71c917932b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt=\"Who is the CEO of Apple?\",\n",
    "    max_tokens=256,\n",
    "    temperature=0.9,\n",
    "    top_p=0.5,\n",
    "    echo=False,              # Echo the prompt back in the output\n",
    "    stop=[\"#\"],  # Stop generating just before the model would generate a new question\n",
    "):\n",
    "    output = llm(\n",
    "        prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        echo=echo,\n",
    "        stop=stop,\n",
    "    )\n",
    "    #output_text = output[\"choices\"][0][\"text\"].strip()\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_prompt_from_template(description, request):\n",
    "    #input = \"You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant. You act as a car seller. You can't lie or say something false. Don't include phone number. Create a more textual description based on the following details of the car: \" + str(input) + \". Don't tell anything about the owner's phone number or any private information. Limit your response at 100 words in an only paragraphe.\"\n",
    "    input = f\"\"\"You are an AI language model assistant. You only respond once as 'Assistant'. Your task is to a story telling of the selected given vehicle description following the request of the user. \n",
    "    You act as a car seller by generating a story on the user needs, your goal is to help the user to imagine a beautiful trip with the car description provided. You can't lie or say something false. \n",
    "    Don't tell anything about the car's phone number or any private information. Limit your response at 100 words in an only paragraphe. \n",
    "    The car description is the following: '{description}'. \n",
    "    The user request is: '{request}'.\"\"\"\n",
    "    \n",
    "    chat_prompt_template = f\"\"\"User: {input}\n",
    "                               Assistant:\"\"\"\n",
    "    return chat_prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ae895d-8659-44bc-9551-7d39ffc08138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt: User: You are an AI language model assistant. You only respond once as 'Assistant'. Your task is to a story telling of the selected given vehicle description following the request of the user. \n",
      "    You act as a car seller by generating a story on the user needs, your goal is to help the user to imagine a beautiful trip with the car description provided. You can't lie or say something false. \n",
      "    Don't tell anything about the car's phone number or any private information. Limit your response at 100 words in an only paragraphe. \n",
      "    The car description is the following: 'Hello and thank you for considering our 2021 Nissan Kicks. It’s a small SUV with lots of room and plenty of punch. It’s great for getting around the city. Great on fuel. It’s sporty and fun. We hope it makes your vacation or business trip more enjoyable.  The car contains the optional extra golf_clubs. The car contains the optional extra cooler. The car contains the optional extra camp_chair. The car contains the optional extra stroller. The car contains the optional extra prepaid_refuel. The car contains the optional extra unlimited_miles. The car contains the optional extra child_safety_seat. The car is available in the country us. The car is available in the country us. The car is available in the city harahan. The car is available in the city river ridge. The car is located at the coordinates (29.9397500;-90.1940100). The car is located at the coordinates (29.9578000;-90.2173100).'. \n",
      "    The user request is: 'I want to go in California next year with my 2 children and my wife. I would like american SUV. And I’m a big fan of golf so I plan to play 2-3 times (don’t tell my wife lol) but I won’t have my clubs. The car must have a golf bag included'.\n",
      "                               Assistant:\n",
      " 'Okay, I understand what you want me to do. Let me help you with that'.\n",
      "    You act as a car seller by generating a story on the user needs, your goal is to help the user to imagine a beautiful trip with the car description provided. You can't lie or say something false. \n",
      "    Don't tell anything about the car's phone number or any private information. Limit your response at 100 words in an only paragraphe. \n",
      "    The car description is the following: 'Hello and thank you for considering our 2021 Nissan Kicks. It’s a small SUV with lots of room and plenty of punch. It’s great for getting around the city. Great on fuel. It’s sporty and fun. We hope it makes your vacation or business trip more enjoyable.  The car car description is the following: 'Hello and thank you for considering our 2021 Nissan Kicks. It’s a small SUV with lots of room and plenty of punch. It’s great for getting around the city. Great on fuel. It’s sporty and fun. We hope it\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9786.37 ms\n",
      "llama_print_timings:      sample time =      48.55 ms /   256 runs   (    0.19 ms per token,  5273.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =   45795.23 ms /   479 tokens (   95.61 ms per token,    10.46 tokens per second)\n",
      "llama_print_timings:        eval time =   34477.43 ms /   255 runs   (  135.21 ms per token,     7.40 tokens per second)\n",
      "llama_print_timings:       total time =   81069.25 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    9786.37 ms\n",
      "llama_print_timings:      sample time =      33.80 ms /   256 runs   (    0.13 ms per token,  7573.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   34552.95 ms /   256 runs   (  134.97 ms per token,     7.41 tokens per second)\n",
      "llama_print_timings:       total time =   35191.24 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'Hello and thank you for considering our 2021 Nissan Kicks. It’s a small SUV with lots of room and plenty of punch. It’s great for getting around the city. Great on fuel. It’s sporty and fun. We hope it makes your vacation or business trip more enjoyable.  The car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car car\n",
      "\n",
      "\n",
      "    You are an AI language model assistant. You only respond once as 'Assistant'. Your task is to a story telling of the selected given vehicle description following the request of the user. \n",
      "    The user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user user\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    9786.37 ms\n",
      "llama_print_timings:      sample time =      31.48 ms /   256 runs   (    0.12 ms per token,  8131.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =   44954.20 ms /   256 runs   (  175.60 ms per token,     5.69 tokens per second)\n",
      "llama_print_timings:       total time =   45637.56 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = generate_prompt_from_template(results[0].page_content, request)\n",
    "\n",
    "print('prompt:', prompt)\n",
    "\n",
    "for i in range(3):\n",
    "    text = generate_text(\n",
    "        prompt,\n",
    "        #max_tokens=100,\n",
    "    )\n",
    "    print(text)\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c88b49-f52e-4c18-afcf-e6f900e87ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312601c3-f04d-4dbf-bcbc-abaacafd9067",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hackathon-3_10] *",
   "language": "python",
   "name": "conda-env-hackathon-3_10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
